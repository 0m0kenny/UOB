---
title: "Stats and Maths"
output: html_document
date: "2024-10-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
```{r}
library(readr)
library(ggplot2)
library(tidyverse)
options(bitmapType='cairo')
rm(list=ls())

```
Chapter 1 

Sample function used to randomly pick an element from a vector.
Replace function takes from the sample and puts the no back if =true 

```{r}
x <- c(1, 2, 3, 4) #this is the sample space

out <- sample(x, 10000, replace=TRUE) #picks no from x list 10000x and puts it back

x <- c( 1, 2, 2, 3, 4, 1, 6, 7, 8, 10, 5, 5, 1, 4, 9 )
# out1 <- sample(x, 20, replace=FALSE) # will give error as you can only pick elements equal to no of elements in the vector
#out1


hist_out <- hist(out, main = '', xlab = 'Values', ylab = 'Frequency')
#plots a histogram of the values picked

```
Use the sample or sample.int function to simulate values from rolls of an unbiased six-sided die. Show that the distribution of values you obtain is consistent with an unbiased die.

Hint 1: Type ?sample.int in the console to get help on this function.

Hint 2: You may find it useful to use the function table. Type ?table in the console to get help on this function.

```{r}
dice1 <- sample(c(1,2,3,4,5,6), size= 100, replace=TRUE)
dice2 <- sample.int(6, size= 100, replace=TRUE) # sample.int samples through intergers
table(dice1)
table(dice2)
#the table will give the frequency table of each interger/element picked. basically realisation of x)


hist_out <- hist(dice2, main = '', xlab = 'Values', ylab = 'Frequency')
```

MArkov Chains
create transition matrix using the matrix function and adding the data as the probabilities of the states
```{r}
transition_matrix = matrix(c(0.4,0.6, 0.7,0.3), nrow=2, ncol=2, byrow=TRUE) #byrow fills the matrix by row so first 2 no in list will fill the first row then next 2 next row etc 2 elements per row because there nrow=2 which is the no of rows

transition_matrix

```
Suppose I want to simulate a sequence of 30 days and the weather patterns over those days. Assuming that on day 0 it is currently sunny, I can do the following:

```{r}
transition_matrix = matrix(c(0.4,0.6, 0.7,0.3), nrow=2, ncol=2, byrow=TRUE)

initial_state <- 1 #initialise the initial state- 1 for sunny or 2 for rainy

weather_seq <- rep(0,30) #create a vector to store the stimulated values. rep() replicates the elements in a vector so rep(0,30) will replicate 0 thirty times. so this will store a list of 30 element 0.

for (day in 1:30) { #iterates over 30 days
  get_prob <- transition_matrix[initial_state, ] # pick the prob needed from the matrix - selects the row of the matrix only using the initial state variable so if initial state is 1, will call row 1 and store the 2 elements in row 1 as a list.
  next_state = sample(c(1,2), size = 1, prob = get_prob) # based on the prob taken, this will randomly pick either 1 or 2 from the list once 
  weather_seq[day] <- next_state #stores the new state inside the weather seq and replaces it so first iteration of the for loop will replace first 0 in weather seq then 2nd replaces 2nd etc.
}
weather_seq


```

Using the rep and the probability functions
```{r}

initial_state = sample(c(1,2), size = 1, prob = c(0.4, 0.6)) # need to state no of prob according to no of elements and must add up to 1
initial_state

get_prob <- rep(0.16,6)
dice3 <- sample.int(6, size= 100, replace=TRUE, prob=get_prob)
table(dice3)
```
Can you extend this example to a three-state model?

adding cloudy 
Note, the diagram (intentionally) misses out the self-transitions. You should be able to infer this because the probabilities given would otherwise not add up to one!

```{r}
transition_matrix2 = matrix(c(0.7,0.2,0.1,0.3,0.3,0.4,0.6,0.2,0.2), nrow=3, ncol=3, byrow=TRUE) #draw the matrix out by hand first
initial_state <- 1
weather_seq2 <- rep(0,30) 

for (day in 1:30) { #iterates over 30 days
  get_prob <- transition_matrix2[initial_state, ] 
  next_state = sample(c(1,2,3), size = 1, prob = get_prob)  
  weather_seq2[day] <- next_state 
}
weather_seq2

```
You could also use a data.frame to store the day and the weather state if you wanted to output the day and the weather state together.  

```{r}
transition_matrix2 = matrix(c(0.7,0.2,0.1,0.3,0.3,0.4,0.6,0.2,0.2), nrow=3, ncol=3, byrow=TRUE) #draw the matrix out by hand first
initial_state <- 1
weather_seq2 <- rep(0,30) 


for (day in 1:30) { #iterates over 30 days
  get_prob <- transition_matrix2[initial_state, ] 
  next_state = sample(c(1,2,3), size = 1, prob = get_prob) 
  weather_seq2[day] <- next_state 
  day_weather <- data.frame(days=(1:30), weather=(weather_seq2))
}
weather_seq2
day_weather
```
Even better would to create plot of the weather sequence over the 30 days or perform basic statistics on the weather sequence to output a useful summary of the weather sequence.

```{r}
transition_matrix2 = matrix(c(0.7,0.2,0.1,0.3,0.3,0.4,0.6,0.2,0.2), nrow=3, ncol=3, byrow=TRUE) #draw the matrix out by hand first
initial_state <- 1
weather_seq2 <- rep(0,30)


for (day in 1:30) { #iterates over 30 days
  get_prob <- transition_matrix2[initial_state, ] 
  next_state = sample(c(1,2,3), size = 1, prob = get_prob) 
  weather_seq2[day] <- next_state 
}
day_weather <- data.frame(days=(1:30), weather=(weather_seq2))

conditions <- c(1,2,3)
replacement_values <- c("Sunny", "Rainy", "Cloudy")
 
day_weather$weather <- replace(day_weather$weather, day_weather$weather %in% conditions, replacement_values)
day_weather
#hist(weather_seq2, main = '', xlab = 'Values', ylab = 'Frequency')

```
Moving around the board
A Monopoly board has 40 spaces. Players take it in turns to roll two dice and traverse around the board according to the sum of the dice values. You can plan this code by considering the main components:

the board
the dice
the player position
the input you need to provide
the output you want to get
Use the following code example to simulate turns of a single player moving around the board. The code will simulate a player moving around the board for a number of turns and store the board positions visited. Consider each step of the code and try to understand what it is doing and why.

```{r}
no_of_turns = 1000 #no of turns
current_pos = 0 #current position on board, start line so 0
move_size <- rep(0, no_of_turns) # replicate the no of steps to take which is no of turns
positions_visited <- rep(0, no_of_turns) 

for (turn in 1:no_of_turns) { # iterate over no of turns

  dice <- sample(c(1:6), 2, replace = TRUE) # randomly select a value stimulating a die roll twice

  no_of_steps <- sum(dice) #add the die values to get no of steps to move

  new_board_pos <- current_pos + no_of_steps #get the position by adding no of steps to current position

  # update board position (this corrects for the fact the board is circular)
  current_pos <- (new_board_pos %% 40)

  # store position visited
  positions_visited[turn] <- current_pos #stores the current position for each iteration into the position visited according to iteration no in for loop

}


```

By increasing the number of turns taken, what distribution does the set of simulated board positions converge towards? Show this graphically using the histogram function.

```{r}
no_of_turns = 1000 #no of turns
current_pos = 0 #current position on board, start line so 0
move_size <- rep(0, no_of_turns) # replicate the no of steps to take which is no of turns
positions_visited <- rep(0, no_of_turns) 

for (turn in 1:no_of_turns) { # iterate over no of turns

  dice <- sample(c(1:6), 2, replace = TRUE) # randomly select a value stimulating a die roll twice

  no_of_steps <- sum(dice) #add the die values to get no of steps to move

  new_board_pos <- current_pos + no_of_steps #get the position by adding no of steps to current position

  # update board position (this corrects for the fact the board is circular)
  current_pos <- (new_board_pos %% 40)

  # store position visited
  positions_visited[turn] <- current_pos #stores the current position for each iteration into the position visited according to iteration no in for loop

}

hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)
```

3.2 Questions to consider
What is the distribution of board positions during a long game?
Can you explain this result qualitatively?
How does the distribution change if you increase the number of turns?
Why do you need to specify 41 of breaks in the histogram?
What does right = FALSE do in the hist function?
```{r}


```

Now we will add the next level of complexity to the simulation. We will consider the possibility of going to jail.

If a player lands on to Go To Jail space they must move immediately to the Jail space. Extend your code to include the possibility of going to jail. Here, assume that once in jail, the player continues as normal on the next turn. This is of course not the case in the real game, but we are simplifying the rules for this simulation.
```{r}
no_of_turns = 1000 #no of turns
current_pos = 0 #current position on board, start line so 0
move_size <- rep(0, no_of_turns) # replicate the no of steps to take which is no of turns
positions_visited <- rep(0, no_of_turns) 
go_to_jail_pos <- 30 # the go to jail position on the board
jail_pos <- 10 # the jail 


for (turn in 1:no_of_turns) { # iterate over no of turns

  dice <- sample(c(1:6), 2, replace = TRUE) # randomly select a value stimulating a die roll twice

  no_of_steps <- sum(dice) #add the die values to get no of steps to move

  new_board_pos <- current_pos + no_of_steps #get the position by adding no of steps to current position
 
   # if land on GO TO JAIL square, then go backwards to the JAIL square
  if (new_board_pos == go_to_jail_pos) {
    new_board_pos <- jail_pos
  }

  # update board position (this corrects for the fact the board is circular)
  current_pos <- (new_board_pos %% 40)

  # store position visited
  positions_visited[turn] <- current_pos #stores the current position for each iteration into the position visited according to iteration no in for loop

}

hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)
```

3.5 Exercise - Repeated doubles
Now we will add the next level of complexity to the simulation by considering the possibility of going to jail with three doubles. A double is when both dice have the same value.

Update your code to allow for the possibility of going to Jail with three doubles. How does the distribution of board positions change?

Plan your code by considering the main components:

detecting doubles
counting doubles
going to jail with three doubles
updating the board position
```{r}
no_of_turns = 1000 #no of turns
current_pos = 0 #current position on board, start line so 0
move_size <- rep(0, no_of_turns) # replicate the no of steps to take which is no of turns
positions_visited <- rep(0, no_of_turns) 
go_to_jail_pos <- 30 # the go to jail position on the board
jail_pos <- 10 # the jail 


for (turn in 1:no_of_turns) { # iterate over no of turns
   # set double counter to zero
  double_counter <- 0

  
  for (j in 1:3){ # roll dice 3 times

    
    dice <- sample(c(1:6), 2, replace = TRUE) # roll two dice
   
    if ((dice[1] == dice[2]) & (double_counter == 2)) { #checks if u have already rolled a double twice 
      current_board_pos <- jail_pos #go to jail 
      break #stops the for loop if double counter is 2 as this means you are on the 3rd go
    }

  
    no_of_steps <- sum(dice) 

    new_board_pos <- current_pos + no_of_steps 
 
   
    if (new_board_pos == go_to_jail_pos) {
      new_board_pos <- jail_pos
    }
  

    current_pos <- (new_board_pos %% 40)
  
     
    if (dice[1] != dice[2]) {  
      break #stops the for loop rolling again if its not a double
    } else { #if its a double, for loop rolls again 
      double_counter <- double_counter + 1 # add 1 to the double counter
    }

  }

  # store position visited
  positions_visited[turn] <- current_pos #stores the current position for each iteration into the position visited according to iteration no in for loop

  
}

hist(positions_visited, breaks = seq(0, 40, len = 41), right = FALSE)


```
3.6 Exercises - Extend the game
As the final part of this exercise, consider building a more complex Monopoly simulation by incorporating more complex aspects of the game such as:

the purchase of properties
a ledger for each player
chance and community cards
You will need to think carefully about the simplifying assumptions you will make to make the task achievable. Do not be over-ambitious. For example, you might initially assume that players will not build houses/hotels on properties.

Here are some questions to answer with your simulations:

How many turns does it take before all properties are purchased?
What are the best properties to buy?
How long does it take for a winner to be determined?
As before you want to plan out your code and break it down into smaller parts. You can start by simulating a single player purchasing properties. You can then extend this to multiple players and so on. Do not try to do everything at once.

```{r}


```
Monte Carlo Integration

set up a simulation, this time to estimate the integral of a Normal distribution. We will go step by step to understand the process and then we will write a function to do this for us.

Define the number of samples we will obtain.
Use the rnorm()  function to simulate 
1,000 numbers from a Normal distribution with mean 1 and standard deviation 2.
Estimate the integral between 1 and 3 by counting how many samples had a value in this range. We can do this by summing the number of samples that are greater than or equal to 1 and less than or equal to 3 and dividing by the total number of samples

```{r}
n <- 1000 # number of samples to take
sims <- rnorm(n, mean = 1, sd = 2) # rnorm() simulates 1,000 no from a normal dist. with mean 1 and sd 2.
mc_integral <- sum(sims >= 1 & sims <= 3)/n #add number of samples from sims that are greater than or equal to 1 and less than or equal to 3 and divide by n
mc_integral


```
We can compare this mc_integral (estimate of the integral) to the true value of the integral which we can calculate using the pnorm() function in R. This is the probability of a Normal distribution falling between 1 and 3.



```{r}
n <- 1000 
sims <- rnorm(n, mean = 1, sd = 2) 
mc_integral <- sum(sims >= 1 & sims <= 3)/n
mc_exact <- pnorm(q = 3, mean = 1, sd = 2) - pnorm(q = 1, mean = 1, sd = 2) #pnorm() gives the integral under the Normal distribution from negative infinity up to q value. first q=3 and 2nd q=1 because estimating integral btw 3 and 1.
mc_exact

```
4.2 Exercise: MC accuracy
Let us now explore how the accuracy of the Monte Carlo estimate changes as the number of samples increases. We will repeat the above process for different numbers of samples and compare the accuracy of the estimate to the true value.

Try increasing the number of simulations over multiple samples sizes and see how the accuracy improves?


```{r}

n <- c(4, 700, 1500, 3000, 7000, 10000) #sample sizes
n_sim <- length(n) #initialise index of n 
mc_exact <- pnorm(q = 3, mean = 1, sd = 2) - pnorm(q = 1, mean = 1, sd = 2)
mc_integral <- rep(0, n_sim) #initialise empty mc_integral list
no_of_reps = 100 #no of times to repeat simulation per sample size

for (reps in no_of_reps){
  for (i in 1:n_sim) { #iterate over n_sim is is 5 times
     sims <- rnorm(n[i], mean =1, sd =2) #correspond each index with the value in n
     
     mc_integral[i] <- sum(sims >= 1 & sims <= 3)/n[i] #add each new integral into position i in mc_integral
     accuracy[i] <- mean(mc_exact - mc_integral[i]) #gets the average of each mc_integral and stores it in accuracy
  }
}


mc_integral
mc_exact
accuracy


```
Can you draw a graph of number of MC samples vs accuracy?

```{r}
n <- c(4, 700, 1500, 3000, 7000, 10000)
mc_exact <- pnorm(q = 3, mean = 1, sd = 2) - pnorm(q = 1, mean = 1, sd = 2)
n_sim <- length(n)
mc_integral <- rep(0, n_sim) 
accuracy <- rep(0, n_sim)
no_of_reps = 100


for (i in 1:n_sim) { 
    sims <- rnorm(n[i], mean =1, sd =2) 
    for (j in 1:no_of_reps) {
    mc_integral[i] <- sum(sims >= 1 & sims <= 3)/n[i] 
    }
    accuracy[i] <- mean(mc_integral[i] - mc_exact)
  }

my_data <- data.frame(no_of_samples=(n), accuracy=(accuracy))
 
ggplot(my_data, aes(x = no_of_samples, y = accuracy)) + 
  geom_point()+ geom_smooth(method = "lm")


mc_integral
mc_exact
accuracy
my_data

```
solution
```{r}

sample_sizes <- c(4, 700, 1500, 3000, 7000, 10000) # try different sample sizes
n_sample_sizes <- length(sample_sizes) # number of sample sizes to try
rpts <- 100 # number of repeats for each sample size
accuracy <- rep(0, n_sample_sizes) # vector to record accuracy values
accuracy_sd <- rep(0, n_sample_sizes) # vector to record accuracy sd values

# Let's store the exact value of the integral
mc_exact <- pnorm(q = 3, mean = 1, sd = 2) - pnorm(q = 1, mean = 1, sd = 2)

# for each sample size
for (i in 1:n_sample_sizes) {

  sample_sz <- sample_sizes[i] # select a sanmple size to use

  # vector to store results from each repeat
  mc_integral <- rep(0, rpts)
  for (j in 1:rpts){
    # simulated normally distributed numbers
    sims <- rnorm(sample_sz, mean = 1, sd = 2)
    # find proportion of values between 1-3
    mc_integral[j] <- sum(sims >= 1 & sims <= 3) / sample_sz
  }

  # compute average difference between integral estimate and real value
  accuracy[i] <- mean(mc_integral - mc_exact)
  # compute sd difference between integral estimate and real value
  accuracy_sd[i] <- sd(mc_integral - mc_exact)

}

accuracy
df <- data.frame(sample_sizes, accuracy, accuracy_sd)
ggplot(df, aes(x = sample_sizes, y = accuracy)) +
  geom_line() +
  geom_point()+
    geom_errorbar(
      aes(ymin = accuracy - accuracy_sd, ymax = accuracy + accuracy_sd),
          width = .2,
          position = position_dodge(0.05)) +
  ylab("Estimate-Exact") +
  xlab("Run")
 
```
Approximating the Binomial Distribution


Now we will consider a different problem, bust using the same techniques. Suppose we have a fair coin and we want to know the probability of getting more than 3 heads in 10 flips. This is a trivial problem using the Binomial distribution but suppose we have forgotten about this or never learned it in the first place.

Lets solve this problem with a Monte Carlo simulation. We will use the common trick of representing tails with 0 and heads with 1, then simulate 10 coin tosses 100 times and see how often that happens.

```{r}
runs <- 100 # number of simulations to run

greater_than_three <- rep(0, runs) # vector to hold outcomes


for (i in 1:runs) {# iterate 100 simulations

  coin_flips <- sample(c(0, 1), 10, replace = T)# flip a coin ten times (0 - tail, 1 - head)

  greater_than_three[i] <- (sum(coin_flips) > 3)# count how many heads and check if greater than 3
}


pr_greater_than_three <- sum(greater_than_three) / runs #get average 
pr_greater_than_three 
```
we can compare the pr_greater_than_three to R’s built-in Binomial distribution function:

```{r}
pbinom(3, 10, 0.5, lower.tail = FALSE) #built in binomal dist.
```
4.4 Problem: MC Binomial

Let’s expand this once again trying to see the impact of the number of simulations on the accuracy of the estimate.

Try increasing the number of simulations and see how the accuracy improves?

Can you plot how the accuracy varies as a function of the number of simulations?

(Hint: see the previous section, the solution is very similar)
```{r}
runs <- c(4, 700, 1500, 3000, 7000, 10000) 
n_runs <- length(runs)
greater_than_three <- rep(0, n_runs) 
pbinom(3, 10, 0.5, lower.tail = FALSE)
accuracy <- rep(0, n_runs)

for (i in 1:n_runs) {

  coin_flips <- sample(c(0, 1), 10, replace = T)

  greater_than_three[i] <- (sum(coin_flips) > 3)
  accuracy[i] <- greater_than_three[i] - mc_exact

}
accuracy
my_data <- data.frame(no_of_runs=(runs), accuracy=(accuracy))
 
ggplot(my_data, aes(x = no_of_runs, y = accuracy)) + 
  geom_point()+ geom_smooth(method = "lm")


pr_greater_than_three <- sum(greater_than_three) / runs
pr_greater_than_three 


```
4.6 Exercise: MC Expectation 1
We can also ask more challenging questions. Let us consider the following problem:

After 20 spins what is the probability that you will have less than 0 points?

How might we solve this?

Of course, there are methods to analytically solve this type of problem but by the time they are even explained we could have already written our simulation!

```{r}
runs <- 100
less_than_o <- rep(0, runs) 


for (i in 1:runs) {

  spin <- sample(c(-1, 1, 2), 20, replace = T, prob = c(0.25,0.5,0.25))
  less_than_o[i] <- (sum(spin))
}


pr_less_than_o <- sum(less_than_o) / runs 
pr_less_than_o

```
# 5 Maximum Likelihood 
The maximum likelihood approach chooses the value of the mean that maximises the likelihood as the estimate

5.1 The likelihood function - computing the likelihood of different values of the mean(mu-greek u) when it is unknown but using log.

First, we are going to write a function to compute the log-likelihood function given parameters: sd, mean, log

This function returns the -sum(logF) because the numerical optimisation algorithm we are going to use finds the minimum of a function. We are interested in the maximum likelihood but we can turn this into a minimisation problem by simply negating the likelihood.

```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84) #data 

n = length(x)

neglogLikelihood <- function(mu, x) { #log-likelihood function -to find the joint probability for independent measurements and then give the log value in negative form. mu(the mean in greek)
  logF = dnorm(x, mean = mu, sd = 1, log = TRUE) #finds the density function(f(x) (pdf) of the data and returns the log of the pdf
  return(-sum(logF)) #gives the sum of the logF in negative form
}



```
5.2 Optimisation
Now, we will need to define an initial search value for the parameter(mu_init), we will arbitrarily pick a value.

Now we will use the R function optim to find the maximum likelihood estimate. As mentioned above, optim finds the minimum value of a function so in this case we are trying to find the parameter that minimises the negative log likelihood.

we will start the search at mu_init using the function neglogLikelihood that we have defined above. The optim algorithm will use the L-BFGS-B search method. The parameter is allowed to take any value from lower = -Inf to upper = Inf. The result is stored in out.
```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

neglogLikelihood <- function(mu, x) { 
  logF = dnorm(x, mean = mu, sd = 1, log = TRUE)
  return(-sum(logF)) 
}

mu_init = 1.0 # define an initial search value for the parameter, we will arbitrarily pick a value . can choose any value for the first mean as the optim() just needs a value to start with then it will pick other values randomly.

out <- optim(mu_init, neglogLikelihood, gr = NULL, x, method = "L-BFGS-B", lower = -Inf, upper = Inf) #to find the max likelihood estimate.  optim() finds the minimum value of a function so in this case we are trying to find the parameter that minimises the negative log likelihood. which is why the loglikelihood is given as a negative. inf means infinite so the lower and upperbounds can use any values between infinity the lower and upper bounds needed because of the method type. 

#optim stores  the resulting data in a datatset. has a par row that stores the value we want

out
out$par #see what parameter values it has found


```
It turns out that it is theoretically known that the maximum likelihood estimate, for this particular problem, is the sample mean which is why they coincide!

```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

neglogLikelihood <- function(mu, x) { 
  logF = dnorm(x, mean = mu, sd = 1, log = TRUE)
  return(-sum(logF)) 
}

mu_init = 1.0 
out <- optim(mu_init, neglogLikelihood, gr = NULL, x, method = "L-BFGS-B", lower = -Inf, upper = Inf)


out$par 

mean(x) #find the mean of x

```
We can visualise this further. First we define an array of possible values for mu in this case between -0.1 and 0.3 with 101 values in-between.
We use the apply function to apply the logLikelihood function to each of the mu values we have defined. This means we do not need to use a for loop.
We can then plot and overlay the maximum likelihood result.
The plot shows that optim has found the mu which minimises the negative log-likelihood.
```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)
neglogLikelihood <- function(mu, x) { 
  logF = dnorm(x, mean = mu, sd = 1, log = TRUE)
  return(-sum(logF)) 
}

mu_init = 5.0 
out <- optim(mu_init, neglogLikelihood, gr = NULL, x, method = "L-BFGS-B", lower = -Inf, upper = Inf)

out

out$par

mu <- seq(-0.1, 0.3, length = 101) # gives 101 no between -0.1 and 0.3

neglogL <- apply( matrix(mu), 1, neglogLikelihood, x) # make mu a matrix then apply the neglogliklihood function on the values stored in mu

plot(mu, neglogL, pch="-") #plot the neglogL values (y-axis) against the mu (x-axis)
points(out$par, out$value, col="red", pch=0) #find and highlight on the graph the par (x-axis) and value (y-axis) found in the out datatset

```
5.3 Two-parameter estimation
Now suppose both the mean and the variance of the Normal distribution are unknown and we need to search over two parameters for the maximum likelihood estimation.

We now need a modified negative log-likelihood function

```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

neglogLikelihood2 <- function(theta,x) {
  mu <- theta[1] # index 1 of theta will be stored as mu
  sigma2 <- theta[2] # index  of theta will be stored as sigma2
  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE) # compute density for each data element in x
  return(-sum(logF)) # return negative log-likelihood
}
```
Notice that we pass through one argument theta whose elements are the parameters for mu and sigma2 which we unpack within the function.

Now we can run optim but this time the initial parameters values must be initialised with two values. Furthermore, as variance cannot be negative, we bound the possible lower values that sigma2 can take by setting lower = c(-Inf, 0.001). The second argument means sigma2 cannot be lower than 0.001

```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

theta_init = c(1, 1) #initialise with the initial parameters

neglogLikelihood2 <- function(theta,x) {
  mu <- theta[1] 
  sigma2 <- theta[2]
  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE)
  return(-sum(logF)) # return negative log-likelihood
}


out <- optim(theta_init, neglogLikelihood2, gr = NULL, x, method = "L-BFGS-B", lower = c(-Inf, 0.001), upper = c(Inf, Inf)) #find max likelihood estimate and prevent lower interval from being negative or lower than 1

```
We can now visualise the results by creating a two-dimensional contour plot. We first need to generate a grid of values for mu and sigma2.

Then apply our negative log-likehood function to this grid to generate a negative log-likelihood value for each position on the grid.


```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

theta_init = c(1, 1) 

neglogLikelihood2 <- function(theta,x) {
  mu <- theta[1] 
  sigma2 <- theta[2]
  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE)
  return(-sum(logF)) 
}


out <- optim(theta_init, neglogLikelihood2, gr = NULL, x, method = "L-BFGS-B", lower = c(-Inf, 0.001), upper = c(Inf, Inf)) 

mu <- seq(-0.1, 1.0, length = 101) #creates a 1D table of  101 values between -0.1 to 1 
sigma2 <- seq(0.1, 1.0, length = 101) #creates a 1D table of 101 values between 0.1 and 1

mu_xx <- rep(mu, each = 101) # replicate the mu 1D table 101 times

sigma2_yy <- rep(sigma2, times = 101) # replicate the sigma2 table 101 times

mu_sigma_grid <- cbind(mu_xx, sigma2_yy) # generate grid of values (each row contains a unique combination of mu and sigma2 values)

neglogL2 <- apply(mu_sigma_grid, 1, neglogLikelihood2, x) #apply the negloglikelihood to the grid



```

Then now use the contour function to plot our results.


```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

theta_init = c(1, 1) 

neglogLikelihood2 <- function(theta,x) {
  mu <- theta[1] 
  sigma2 <- theta[2]
  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE)
  return(-sum(logF)) 
}


out <- optim(theta_init, neglogLikelihood2, gr = NULL, x, method = "L-BFGS-B", lower = c(-Inf, 0.001), upper = c(Inf, Inf)) 

mu <- seq(-0.1, 1.0, length = 101) 
sigma2 <- seq(0.1, 1.0, length = 101) 

mu_xx <- rep(mu, each = 101) 

sigma2_yy <- rep(sigma2, times = 101) 

mu_sigma_grid <- cbind(mu_xx, sigma2_yy)

neglogL2 <- apply(mu_sigma_grid, 1, neglogLikelihood2, x) 


neglogL2 <- matrix(neglogL2, 101) # convert vector of negative log-likelihood values into a grid


contour(sigma2, mu, neglogL2, nlevels = 50, xlab = "sigma2", ylab = "mu") # draw contour plot

points(out$par[2], out$par[1], col="red") #find and highlight on the graph the par (x-axis) and value (y-axis) found in the out datatset

```

We have now found the maximum likelihood estimates for the unknown mean and variance for the Normal distribution that our data is assumed to be drawn from. Let’s compare our estimates against the sample mean and variance. 
Interesting! The maximum likelihood estimates return the sample mean and the biased sample variance estimate (where we normalise by 
n and not n−1). Indeed, it turns out that theoretically, the maximum likelihood estimate does give a biased estimate of the population variance. 
```{r}
x = c(-0.5, 1.0, 0.2, -0.3, 0.5, 0.89, -0.11, -0.71, 1.0, -1.3, 0.84)

n = length(x)

theta_init = c(1, 1) 

neglogLikelihood2 <- function(theta,x) {
  mu <- theta[1] 
  sigma2 <- theta[2]
  logF <- dnorm(x, mean = mu, sd = sqrt(sigma2), log = TRUE)
  return(-sum(logF)) 
}


out <- optim(theta_init, neglogLikelihood2, gr = NULL, x, method = "L-BFGS-B", lower = c(-Inf, 0.001), upper = c(Inf, Inf)) 

mu <- seq(-0.1, 1.0, length = 101) 
sigma2 <- seq(0.1, 1.0, length = 101) 

mu_xx <- rep(mu, each = 101) 

sigma2_yy <- rep(sigma2, times = 101) 

mu_sigma_grid <- cbind(mu_xx, sigma2_yy)

neglogL2 <- apply(mu_sigma_grid, 1, neglogLikelihood2, x) 


neglogL2 <- matrix(neglogL2, 101) 


contour(sigma2, mu, neglogL2, nlevels = 50, xlab = "sigma2", ylab = "mu") 

points(out$par[2], out$par[1], col="red") 

out$par[1] #the mu estimate 
out$par[2] #the sigma2 estimate
mean(x) #sample mean
var(x) #sample variance (normalised by n-1)
var(x)*(n-1)/n # sample variance (normalised by n)
```

5.4 Exercise: MLE
A potentially biased coin is tossed 10 times and the number of heads recorded. The experiment is repeated 5 times and the number of heads recorded was 3, 2, 4, 5 and 2 respectively.

Can you derive a maximum likelihood estimate of the probability of obtaining a head?
```{r}
x= c(3,2,4,5,2)
n = 10
negloglike<- function(p, n, x){
  logpdf<- dbinom(x, n, prob = c(p, 1 - p), log =TRUE) #binomial dist. so find pmf using dbinom()
  return (-sum(logpdf))
  
}

p_init = 0.5 #looking for max likelihood for the probability of getting a head
out <- optim(p_init, negloglike, gr = NULL, n, x, method = "L-BFGS-B", lower = 0.001, upper = 1-0.001) 

p_vals <- seq(0.001, 1 - 0.001, length = 101)

neglog_for_all <- apply(matrix(p_vals), 1, negloglike, n, x)

plot(p_vals, neglog_for_all, pch = "-")
points(out$par, out$value, col = "red", pch = 0)


```


```{r}

```
6 Confidence Intervals
In this practical session, you will learn how to derive confidence intervals for a particular problem using Monte Carlo simulations.

The confidence level tells us the probability that the procedure that is used to construct the confidence interval will result in the interval containing the true population parameter. It is not the probability that the population parameter lies in the range.

Lets define the width of an interval, we will set this to 1 initially but we will change this later on:
```{r}
interval_width <- 1 # width of confidence interval

```

We are now going to generate some simulated data for our experiment. We will create 30 samples from a Normal distribution with mean 2.5 and variance 1. These are true values of the population parameters. In a real experiment, we would not know these values but using simulated data, we obviously control these.

Let define these first
Then generate some normally distributed data using the R function rnorm,
```{r}
interval_width <- 1
n <- 30 # number of data points to generate

mu <- 2.5 # population mean

sigma <- 1.0 # population sd 
x <- rnorm(n, mean = mu, sd = sigma) # randomly generate 30 values from the Normal distribution N(2.5, 1.0)


```

6.3 Constructing the confidence interval
We are going to pretend that we do not know the population mean value (2.5) used to generate this dataset and try to provide an interval estimate for it from the simulated sample data.

Remember, from lectures, that the sample mean xbar is a natural point estimate for the population mean μ.
so a suitable interval might be centred on the sample mean and extend out,
```{r}
interval_width <- 1

n <- 30 
mu <- 2.5 
sigma <- 1.0 

x <- rnorm(n, mean = mu, sd = sigma)

x_bar <- mean(x) # compute sample mean from the random values in x
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2) #get the intervals by plus minus by interval width/2

interval

```
6.5 Experiment
The previous experiment only examined one simulated dataset so we cannot fully understand the probabilistic interpretation of the confidence interval just yet. At the moment, the interval you have calculated will either contain the population mean or not.

In order to understand the probabilistic interpretation, we will need to generate many data sets, construct confidence intervals as we have for each and then see across all generated data sets, how often those intervals cover the true population mean.

For a Monte Carlo simulation, we will need many repeats of the simulation. Lets define the number of repeats to be used:

```{r}
interval_width <- 1

n <- 30 
mu <- 2.5 
sigma <- 1.0 

x <- rnorm(n, mean = mu, sd = sigma)

x_bar <- mean(x) 
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2) 

nreps <- 1000 # number of Monte Carlo simulation to run
```

We will use 1000 simulations initially to make the code quick to run but you may want to make this higher later on for greater accuracy.

Now, let us define a series of interval widths to simultaneously test, then create a vector of zeros of the same length. We will use this to store the number of times that a confidence interval of those specific widths contain the true population mean

```{r}
interval_width <- 1

n <- 30 
mu <- 2.5 
sigma <- 1.0 

x <- rnorm(n, mean = mu, sd = sigma)

x_bar <- mean(x) 
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2) 
nreps <- 1000
interval_width <- seq(0.1, 1.0, 0.1) # define a series of interval widths. This creates a sequence of values from 0.1 to 1.0 in steps of 0.1 in the vector

n_interval_widths <- length(interval_width) # store the length interval widths generated into a list

mu_contained <- rep(0, n_interval_widths) # create a vector to store the number of times the population mean is contained

```
We use a for loop to repeat the simulation nreps times. Within each loop, we will simulate a new data set, compute a sample mean and then check if the confidence interval contains the true population mean. Since we are using more than one confidence width, we use a second for loop to cycle through the different widths.

Then calculate, for each width, an estimate of the probability that a confidence interval of that width will contain the population mean.
```{r}
interval_width <- 1

n <- 30 
mu <- 2.5 
sigma <- 1.0 

x <- rnorm(n, mean = mu, sd = sigma)

x_bar <- mean(x) 
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2) 
nreps <- 1000
interval_width <- seq(0.1, 1.0, 0.1)

n_interval_widths <- length(interval_width) 
mu_contained <- rep(0, n_interval_widths)

for (replicate in 1:nreps) {

  x <- sigma * rnorm(n) + mu # simulate a data set

  xbar <- mean(x) # compute the sample mean

  # for each interval width that we are testing ...
  for (j in 1:n_interval_widths) {
    # check if the interval contains the true mean
    if ((mu > xbar - 0.5 * interval_width[j]) &
        (mu < xbar + 0.5 * interval_width[j])) {
      # if it is, we increment the count by one for this width
      mu_contained[j] <- mu_contained[j] + 1
    }
  }

}

probability_mean_contained <- mu_contained / nreps 

```

Let’s use ggplot2 to plot this relationship,
```{r}
interval_width <- 1

n <- 30 
mu <- 2.5 
sigma <- 1.0 

x <- rnorm(n, mean = mu, sd = sigma)

x_bar <- mean(x) 
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2) 
nreps <- 1000
interval_width <- seq(0.1, 1.0, 0.1)

n_interval_widths <- length(interval_width) 
mu_contained <- rep(0, n_interval_widths)

for (replicate in 1:nreps) {
  x <- sigma * rnorm(n) + mu 
  xbar <- mean(x) 
  for (j in 1:n_interval_widths) {
    if ((mu > xbar - 0.5 * interval_width[j]) &
        (mu < xbar + 0.5 * interval_width[j])) {
      mu_contained[j] <- mu_contained[j] + 1
    }
  }
}

probability_mean_contained <- mu_contained / nreps

# create a data frame containing the variables we wish to plot
df <- data.frame(interval_width = interval_width,
                 probability_mean_contained = probability_mean_contained)

# initialise the ggplot
plt <- ggplot(df, aes(x = interval_width, y = probability_mean_contained))
# create a line plot
plt <- plt + geom_line()
# add a horizontal axis label
plt <- plt + xlab("Interval Width")
# create a vertical axis label
plt <- plt + ylab("Probability that mu is contained")

print(plt) # plot to screen
```

Can you see that an interval width of 
0.6(xbar±0.3) gives a confidence interval close to 90% probability of containing the population mean?

Remember from the lectures that we saw that the theory says xbar±1.65 times σ/√n gives a 90% confidence interval?

So, if we compute 2×1.65 times σ√n, what do we get?
```{r}
n <- 30 
mu <- 2.5 
sigma <- 1.0 

2 * 1.65 * sigma / sqrt(n) #1.65 is from zalpha/2 for 0.90
```
6.6 Exercise: Confidence Interval
Can you devise a way to compute a confidence interval for the population standard deviation?

You can make use of the following as a point estimate of the sample variance:

s^2=1/n−1 x n∑i=1 (x−xbar)^2

which can be calculated using the sd function in R, remember the relationship between the standard deviation and variance.

```{r}
interval_width <- 1

n <- 30 
mu <- 2.5 
sigma <- 1.0 

x <- rnorm(n, mean = mu, sd = sigma)

x_bar <- mean(x) 
interval <- c(x_bar - interval_width / 2, x_bar + interval_width / 2) 
nreps <- 1000
interval_width <- seq(0.1, 1.0, 0.1)

n_interval_widths <- length(interval_width) 
sigma_contained <- rep(0, n_interval_widths)

for (replicate in 1:nreps) {
  x <- sigma * rnorm(n) + mu 
  sigmabar <- sd(x) 
  for (j in 1:n_interval_widths) {
    if ((sigma > sigmabar - 0.5 * interval_width[j]) &
        (sigma < sigmabar + 0.5 * interval_width[j])) {
      sigma_contained[j] <- sigma_contained[j] + 1
    }
  }
}

probability_var_contained <- sigma_contained / nreps

# create a data frame containing the variables we wish to plot
df <- data.frame(interval_width = interval_width,
                 probability_var_contained = probability_var_contained)

# initialise the ggplot
plt <- ggplot(df, aes(x = interval_width, y = probability_var_contained))
# create a line plot
plt <- plt + geom_line()
# add a horizontal axis label
plt <- plt + xlab("Interval Width")
# create a vertical axis label
plt <- plt + ylab("Probability that var is contained")

plt
df

```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```
